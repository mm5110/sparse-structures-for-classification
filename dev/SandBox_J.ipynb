{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sandbox Demo for CSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import sparse_coding_classifier_functions as scc\n",
    "\n",
    "from AuxiliaryFunctions import showFilters\n",
    "\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model Class and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSC CLASSES AND CONSISTENCY FUNCTIONS\n",
    "class SL_CSC_FISTA(nn.Module):\n",
    "    def __init__(self, stride=1, dp_channels=1, atom_r=1, atom_c=1, numb_atom=1, tau=1, T_SC=1, T_PM=1):\n",
    "        super(SL_CSC_FISTA, self).__init__()\n",
    "        self.D_trans = nn.Conv2d(dp_channels, numb_atom, (atom_r, atom_c), stride, padding=0, dilation=1, groups=1, bias=False)\n",
    "        self.D = nn.ConvTranspose2d(numb_atom, dp_channels, (atom_c, atom_r), stride, padding=0, output_padding=0, groups=1, bias=False, dilation=1)\n",
    "        self.normalise_weights()\n",
    "        self.D_trans.weight.data = self.D.weight.data.permute(0,1,3,2)\n",
    "        self.tau = tau\n",
    "        self.T_SC = T_SC\n",
    "        self.T_PM = T_PM\n",
    "        self.forward_type = 'FISTA'\n",
    "\n",
    "    def forward(self, Y):\n",
    "    # Initialise t variables needed for FISTA\n",
    "        t1 = 1\n",
    "        # Initialise X1 Variable - note we need X1 and X2 as we need to use the prior two prior estimates for each update\n",
    "        y_dims = list(Y.data.size())\n",
    "        w_dims = list(self.D_trans.weight.data.size())\n",
    "        # Initialise our guess for X\n",
    "        X1 = Variable(torch.rand(y_dims[0], w_dims[0], (y_dims[2]-w_dims[2]+1),(y_dims[3]-w_dims[3]+1)))\n",
    "        # Calculate first update\n",
    "        X2, FISTA_error, alpha = self.linesearch(Y,X1)\n",
    "\n",
    "        #raise Exception()\n",
    "\n",
    "        # computing max eigenvalue with power method\n",
    "#         for i in range(T_pm):\n",
    "#             X1 = self.D_trans(self.D(X))\n",
    "#             X1 = X1./norm(X1)\n",
    "        \n",
    "        for i in range(self.T_SC):\n",
    "            # Update t variables\n",
    "            t2 = (1 + np.sqrt(1+4*(t1**2)))/2 \n",
    "            # Update Z\n",
    "            Z = X2 + (X2-X1)*(t1 - 1)/t2\n",
    "\n",
    "\n",
    "            # Update variables for next iteration\n",
    "            X1 = X2.clone() #untoggle\n",
    "            t1 = t2\n",
    "            # Print at intervals to present progress\n",
    "            if i==0 or (i+1)%5 == 0:\n",
    "                av_num_zeros_per_image = X2.data.nonzero().numpy().shape[0]/y_dims[0]\n",
    "                percent_zeros_per_image = 100*av_num_zeros_per_image/(y_dims[2]*y_dims[3])\n",
    "                l2_error = np.sum((Y-self.D(X2)).data.numpy()**2)\n",
    "                l1_error = np.sum(np.abs(X2.data.numpy()))\n",
    "                # pix_error = l2_error/(y_dims[0]*y_dims[2]*y_dims[3])\n",
    "                error_percent = l2_error*100/(np.sum((Y).data.numpy()**2))\n",
    "                # print(\"Iteration: \"+repr(i) + \", l2 error:{0:1.2f}\".format(l2_error) + \", l1 error: {0:1.2f}\".format(l1_error) + \", l2 error percent: {0:1.2f}\".format(error_percent)+ \"%, Total FISTA error: {0:1.2f}\".format(FISTA_error) + \", Av. sparsity: {0:1.2f}\".format(percent_zeros_per_image) +\"%\")\n",
    "                print(\"After \" +repr(i+1) + \" iterations of FISTA, average l2 error over batch: {0:1.2f}\".format(error_percent) + \"% , Av. sparsity per image: {0:1.2f}\".format(percent_zeros_per_image) +\"%\")\n",
    "        return X2\n",
    "\n",
    "\n",
    "    def reverse(self, X):\n",
    "        out = self.D(X)\n",
    "        return out\n",
    "\n",
    "# ------------- ------------- ------------- ------------- ------------- ------------- ------------- -------------\n",
    "def linesearch(self,Y,X):\n",
    "\t\t# Define search parameter for Armijo method\n",
    "\t\tc = 0.5\n",
    "\t\talpha = 1\n",
    "\t\tg = self.D_trans(Y-self.D(X))\n",
    "\t\tST_arg = X + alpha*g\n",
    "\t\tX_update = soft_thresh(ST_arg, self.tau*alpha)\n",
    "\t\t# Calculate cost of current X location\n",
    "\t\tl1_error = np.sum(np.abs(X.data.numpy()))\n",
    "\t\tl2_error = np.sum((Y-self.D(X)).data.numpy()**2)\n",
    "\t\tcurrent_cost = l2_error + self.tau*l1_error\n",
    "\t\t# print(\"Cost at the beginning of the linesearch: {0:1.2f}\".format(current_cost)+\", l2 error:{0:1.2f}\".format(l2_error) + \", l1 error: {0:1.2f}\".format(l1_error))\n",
    "\t\t# Calculate the cost of the updated position\n",
    "\t\tupdate_cost = np.sum((Y-self.D(X_update)).data.numpy()**2) + self.tau*np.sum(np.abs(X.data.numpy()))\n",
    "\t\t# While the cost at the next location is higher than the current one iterate\n",
    "\t\tcount = 0\n",
    "\t\twhile update_cost >= current_cost and count<=15:\n",
    "\t\t\talpha = alpha*c\n",
    "\t\t\tST_arg = X + alpha*g\n",
    "\t\t\tX_update = soft_thresh(ST_arg, self.tau*alpha)\n",
    "\t\t\tl1_error = np.sum(np.abs(X_update.data.numpy()))\n",
    "\t\t\tl2_error = np.sum((Y-self.D(X_update)).data.numpy()**2)\n",
    "\t\t\tupdate_cost = l2_error + self.tau*l1_error\n",
    "\t\t\tcount +=1\n",
    "\t\t# print(\"Cost at the end of the linesearch: {0:1.2f}\".format(update_cost)+ \", l2 error:{0:1.2f}\".format(l2_error) + \", l1 error: {0:1.2f}\".format(l1_error))\n",
    "\t\treturn X_update, update_cost, alpha\n",
    "\n",
    "    \n",
    "    \n",
    "\tdef normalise_weights(self):\n",
    "\t\tprint(\"Normalising kernels\")\n",
    "\t\tfilter_dims = list(np.shape(self.D.weight.data.numpy()))\n",
    "\t\tfor i in range(filter_dims[0]):\n",
    "\t\t\tfor j in range(filter_dims[1]):\n",
    "\t\t\t\tl2_norm = ((np.sum(self.D.weight.data[i][j].numpy()**2))**0.5)\n",
    "\t\t\t\tif l2_norm > 10^(-7): \n",
    "\t\t\t\t\tself.D.weight.data[i][j] = self.D.weight.data[i][j]/l2_norm\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"Kernel with 0 l2 norm identified, setting to zero\")\n",
    "\t\t\t\t\tself.D.weight.data[i][j] = torch.zeros(filter_dims[2], filter_dims[3])\n",
    "                    \n",
    "\n",
    "# ------------- ------------- ------------- ------------- ------------- ------------- ------------- ------------- -------------\n",
    "\n",
    "def train_SL_CSC(CSC, train_loader, num_epochs, T_DIC, cost_function, optimizer, batch_size):\t\n",
    "\tprint(\"Training SL-CSC. Batch size is: \" + repr(batch_size))\n",
    "\t# Initialise variables needed to plot a random sample of three kernels as they are trained\n",
    "\tfilter_dims = list(np.shape(CSC.D_trans.weight.data.numpy()))\n",
    "\tidx = random.sample(range(0, filter_dims[0]), 3)\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "# \t\tprint(\"Training epoch \" + repr(epoch+1) + \" of \" + repr(num_epochs))\n",
    "\t\tfor i, (inputs, labels) in enumerate(train_loader):\n",
    "# \t\t\tprint(\"Batch number \" + repr(i+1))\n",
    "            \n",
    "\t\t\tinputs = Variable(inputs)\n",
    "\t\t\tlabels = Variable(labels)\n",
    "\t\t\t# Calculate and update step size for sparse coding step\n",
    "\t\t\tinput_dims = list(inputs.size())\n",
    "\t\t\t# Fix dictionary and calculate sparse code\n",
    "\t\t\tif CSC.forward_type == 'FISTA_fixed_step':\n",
    "\t\t\t\tCSC.calc_L(input_dims)\n",
    "\t\t\tX = CSC.forward(inputs)\n",
    "\t\t\t# Fix sparse code and update dictionary\n",
    "\t\t\tprint(\"Running dictionary update\")\n",
    "\t\t\tfor j in range(T_DIC):\n",
    "\t\t\t\t# Zero the gradient\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t# Calculate estimate of reconstructed Y\n",
    "\t\t\t\tinputs_recon = CSC.reverse(X)\n",
    "\t\t\t\t# Calculate loss according to the defined cost function between the true Y and reconstructed Y\n",
    "\t\t\t\tloss = cost_function(inputs_recon, inputs)\n",
    "\t\t\t\t# Calculate the gradient of the cost function wrt to each parameters\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\t# Update each parameter according to the optimizer update rule (single step)\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\t# At the end of each batch plot a random sample of kernels to observe progress\n",
    "\t\t\t\tif j==0 or (j+1)%20 == 0:\n",
    "\t\t\t\t\tprint(\"Average loss per data point at iteration {0:1.0f}\".format(j+1) + \" of SGD: {0:1.4f}\".format(np.asscalar(loss.data.numpy())))\n",
    "\t\t\t\t\tplt.figure(1)\n",
    "\t\t\t\t\tplt.subplot(1,3,1)\n",
    "\t\t\t\t\tplt.imshow((CSC.D.weight[idx[0]][0].data.numpy()), cmap='gray')\n",
    "\t\t\t\t\tplt.title(\"Filter \"+repr(idx[0]))\n",
    "\t\t\t\t\tplt.subplot(1,3,2)\n",
    "\t\t\t\t\tplt.imshow((CSC.D.weight[idx[1]][0].data.numpy()), cmap='gray', )\n",
    "\t\t\t\t\tplt.title(\"Filter \"+repr(idx[1]))\n",
    "\t\t\t\t\tplt.xlabel(\"Epoch Number: \" + repr(epoch)+ \", Batch number: \" + repr(i+1) + \", Average loss: {0:1.4f}\".format(np.asscalar(loss.data.numpy())))\n",
    "\t\t\t\t\tplt.subplot(1,3,3)\n",
    "\t\t\t\t\tplt.imshow((CSC.D.weight[idx[2]][0].data.numpy()), cmap='gray')\n",
    "\t\t\t\t\tplt.title(\"Filter \"+repr(idx[2]))\n",
    "\t\t\t\t\tplt.draw()\n",
    "\t\t\t\t\tplt.pause(0.001)\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tl2_error_percent = 100*np.sum((inputs-CSC.D(X)).data.numpy()**2)/ np.sum((inputs).data.numpy()**2)\n",
    "\t\t\tprint(\"After \" +repr(j+1) + \" iterations of SGD, average l2 error over batch: {0:1.2f}\".format(l2_error_percent) + \"%\")\n",
    "\t\t\t# Normalise each atom / kernel\n",
    "\t\t\tCSC.normalise_weights()\n",
    "\t\t\t# Ensure that weights for the reverse and forward operations are consistent\t\n",
    "\t\t\tCSC.D_trans.weight.data = CSC.D.weight.data.permute(0,1,3,2)\n",
    "\t# Return trained CSC\n",
    "\treturn CSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-14-36f198c8fc2d>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-36f198c8fc2d>\"\u001b[1;36m, line \u001b[1;32m46\u001b[0m\n\u001b[1;33m    print(\"Training SL-CSC. Batch size is: \" + repr(batch_size))\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Paramters and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 1 #100\n",
    "batch_size = 256\n",
    "T_SC = 50\n",
    "T_DIC = 10\n",
    "T_PM = 8\n",
    "stride = 1\n",
    "learning_rate = 3\n",
    "momentum = 0.9\n",
    "weight_decay=0\n",
    "\n",
    "# Weight importance of sparsity vs. reconstruction\n",
    "tau = 0.9\n",
    "\n",
    "# Local dictionary dimensions\n",
    "atom_r = 28\n",
    "atom_c = 28\n",
    "numb_atom = 100\n",
    "dp_channels = 1 \n",
    "\n",
    "# Load MNIST\n",
    "root = './data'\n",
    "download = False  # download MNIST dataset or not\n",
    "\n",
    "# Access MNIST dataset and define processing transforms to proces\n",
    "# trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = dsets.MNIST(root=root, train=True, transform=trans, download=download)\n",
    "test_set = dsets.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "idx = list(range(10000))\n",
    "train_sampler = SubsetRandomSampler(idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 sampler = train_sampler,# None\n",
    "                 shuffle=False) #True\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "# Intitilise Convolutional Sparse Coder CSC\n",
    "CSC = scc.SL_CSC_FISTA(stride, dp_channels, atom_r, atom_c, numb_atom, tau, T_SC, T_PM)\n",
    "\n",
    "# Define optimisation parameters\n",
    "CSC_parameters = [{'params': CSC.D.parameters()}]\n",
    "\n",
    "# Define training settings/ options\n",
    "cost_function = nn.MSELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(CSC_parameters, lr=learning_rate, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
    "# optimizer = torch.optim.Adam(SSC.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSC = scc.train_SL_CSC(CSC, train_loader, num_epochs, T_DIC, cost_function, optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
