{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code and Problem Diary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date: 17/04/2018, Author: MM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of problem:**\n",
    "Main issue trying to sort out at a high level is the training algorithm not converging... Assuming the algorithm is implemented correctly then there are two levers by which to alter the convergence of the algorith - the $\\tau$ parameter controlling level of sparsity induced by the soft thresholding algorithm and the learning rate of the SGD controlling the step take at each iteration to update the dictionary. Currently we observe the following:\n",
    "- If we set both $\\tau$ and the learning rate to 1 then in the first iteration the sparsity level almost instantly falls to zero. Essentially the l1 error significance is so much larger than the l2 error that the minimum is found by selecting the minimiser of the l1 norm, the zero vector. If this happens in the first iteration then nothing else really matters, our algorithm has reached an equilibrium point from which it cannot escape.\n",
    "- With a $\\tau$ of order $10^{-5}$ and the learning rate still at 1 we finally get a setup where we do not almost immediatly plunge towards the 0 vector. Here we can observe that the l1 error even at 50% sparsity is around 390k while the l2 error term is only around 89k. Hence it is clear why the algorithm is prioritising minimising the l1 error. Interestingly after just two iterations of the whole algorith, e.g. FISTA then dictionary update we reach another equilibrium point, the sparsity level stays at around 63%, while the l2 error increases from around 87k untill it hits around 92k. This implies the learning rate is too high. The stepsize also becomes increasingly small which implies that since $\\tau$ is fixed, 1/L is getting smaller and that hence L is getting large. This means that the largest singular value of the weight matrix is getting increasingly large, perhaps implying that the coefficient values are becoming increasingly large in the weight matrix. This is further evidence that the learning rate is to high and suggests and interestingly that the learning rate needs to be low enough to ensure that L is not to large. Furthermore it might suggest we should do something in terms of normalising the kernel filters in some way. This though one would think interrupts with the parameter learning\n",
    "- Following on from the following bullet point if we use $\\tau$ of order $10^{-5}$ and the learning rate at 0.0001 then the average loss per data point and the loss at each iteration at FISTA sticks. Although one would expect the l2 error to stay the same or maybe even increase within each FISTA step one would expect the error between each round of FISTA to decrease, since between each round the dictionary update should be working exclusively to reduce this error. However clearly the dictionary is failing to adapt to reduce the l2 error and converge to a lower value even with a very small step size. If we boost the learning rate to something silly like 10 then we instantly get back to the state we were in before where we sit at around 63% sparsity but with an l2 error of around 121k.\n",
    "\n",
    "Big problem then seems to be in the dictionary update rather than the FISTA step. Observing the weight update it is clear that the weights are updating overtime, and also as might have been feared, that both $D$ and $D^T$ are consistent. Of note however is that the magnitudes of the weight elements is very small, order $10^{-5}$. Interestingly with tau set to 0, i.e. with no sparisty induced, neither the FISTA or the dictionary update reduce the l2 error term. This is quite curious - intuitively perhaps if the values of the weight matrix are very off then it will make it hard to learn a better X and likewise the converse. However this is far from a rigorously thought through statement! It seems unlikely that Pytorch would incorrectly implement e.g. SGD and code logic seems reasonable, so most likely case is that the FISTA implementation is not correct or we need to use some form of backtracking to update tau to deal with the change in the size of the norm. Next steps:\n",
    "- Implement IHT with k = 20 and see if this works any better\n",
    "- On FISTA try normalising the norms so that they are of equal importance/ controlled fully by $\\tau$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date: 19/04/2018, Author: MM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of problem:**\n",
    "Have now implemented an IHT version of the forward pass single layer network. This seems to work a better but still is not perfect:\n",
    "- With IHT as implemented after a few batches the average l2 error per image is around 0.025-0.04\n",
    "- The FISTA model jumps consistently between 0.075 and 0.1\n",
    "\n",
    "In both case decreasing the learning rate and increasing the number of steps at each dictionary update seems to provide significant benefits over the training over a given batch. However between each batch the l2 reconstruction error jumps back up, in. the case of FISTA to around 0.1 and in the case of IHT to around 0.06.\n",
    "\n",
    "The big problem iwth FISTA seems to be the $\\tau$ parameter; at the beginning if the tau parameter is too large then it simply sets the representation to 0. This is because the singular values of the weight matrix must be very small which makes sense for a random initialisation. However as the training progresses the L values grow and hence the stepsize $\\tau/ L$ becomes entirely innapropriate. At this point sparsity fails to be enforced / encouraged so the sparsity level stays at around 64%. Oddly enough though the l2 error remains pretty high, around the 80-90k mark total which is curious, it should be able to get a lower l2 error than IHT since it is essentially only optimising over the l2. This happens because the update for for the ST argument is multiplied by the step size as well remember! This is very very small, so essentially we are just thresholding the same tensor, with the same threshold value. As a result because the stepsize essentially goes to zero then FISTA gets stuck in a cycle. ** *To fix the FISTA implementation need to update or manage the step size better with something like e.g. backtracking* **.\n",
    "\n",
    "With regards to what is stopping IHT progressing So what happens is the following:\n",
    "- a batch comes in and the 'forward pass' solves a sparse coding problem to generate a sparse representation. The algorithm for doing this is iterative; with FISTA we iteratively converge towards the minimum of the convex non-smooth objective function, with the IHT as implemented we fix the number of nonzeros and iterate to improve the l2 error on the reconstruction (I assume?!). Weirdly however at the moment although for the first few batches the l2 error decreases, after a few batches the l2 error increases over the iterations of the IHT! Something is amiss there\n",
    "- For this given batch then we fix the representation that we found through the sparse coding and try to optimise the weights to improve the l2 error on the reconstruction. We do this by running a number of gradient descent steps, for both IHT and FISTA this step seems to work well! The error decreases a lot. At the end the weights for the forward and backward pass are updated to be consistent\n",
    "- then a new batch comes in, the dictionary naturally hasn't been optimised for this batch, so the initial error is higher than it was at the end of the last batch. Run the sparse coding again etc. and continue.\n",
    "\n",
    "The idea is that overtime the distance between the error at the end of the last dictionary update and the initial error at the beginning of the next batch converges towards 0. ** *To fix the IHT need to solve* **:\n",
    "- the initial error for each new batch keeps jumping up again. A number of solutions for this are the following,  increase batch size, just train for longer, make sure not overfitting the dictionary as it where to a particular batch. ** *The most promising lines then is regard seem be just increase batch size and train for longer then, once we have a really low l2 error can worry about generalisation!* **\n",
    "- ** *IHT error seems to be increasing with each iteration! This needs to be fixed* **\n",
    "\n",
    "\n",
    "Problem identifief with the hardthresholding function, have not searched over the largest absolute values.\n",
    "\n",
    "\n",
    "**Post chat with JT**\n",
    "After chatting with Jared next steps:\n",
    "- Normalise the filters so that they have l2 norm of 1. This should not affect things since when the sparse code is next computed it should be taken care of.\n",
    "- FISTA: add backtracking on the lambda variable, ask Jere again what he did here, trying to recreate Jere's results\n",
    "- IHT: still need to ensure that error is not increasing with each iteration, this may be due to the stepsize.\n",
    "- Implement NIHT: need to calculate gamma term each time, may best to have as one combined class\n",
    "\n",
    "Problem: now that I have normalised the atoms of the dictionary to be one, the IHT algorithm seems to explode... Observing the l2 norm of each weight it is 1 as intended. What is essentially happening is that the l2 error of the reconstruction is exploding with each iteration of either FISTA or NIHT. Printing out the largest coefficient per image across all filters the size of the filter grows exponentially per iteration. i.e. every iteration the magnitude of X is growing. Printing out the l2 norm of the sparse rep of a given image it grows exponentially. starting at 0 after a few iterations its around 700. It is also not just IHT, FISTA estimate also seems to explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date: 20/04/2018, Author: MM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main effort this morning is focused around cracking the explosion in the l2 error of the signals X. Not entirely sure why the normalisation is causing such problems, believe the issue lies with FISTA and IHT implementations. The actual implementations seem to be ok: hence I think it is concerned with the values of the hyperparameters, i.e. the step sizes.\n",
    "\n",
    "Note that for IHT the algorithm is only guaranteed to converge if the operator norm is less than one, which is not the case for normalised columns. A better more general algorithm to leverage is NIHT, where we apply a step size calculated in a specific way. Therefore plan is to implement NIHT and FISTA with backtracking to see if this solves the l2 error explosion. \n",
    "\n",
    "After implementing backtracking on FISTA seems to instantly set everything to zero. The cause for this is simple, randomly initialising X means that both the reconstruction error and the l1 norm component are high. Sadly with x set to zero the l2 error and the l1 error are lower, so it instantly jumps to this since, if we use Armijio method, we start at a high stepsize and stop when the error is lower than currently. As a result for an X that is initialised very small, any large value of alpha will zero all entries of X, and this new X will be selected. Once this X is zero it is hard to escape since our alpha gets progressively smaller and smaller and hence the updated attempt of try of X gets closer and closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date 23/04/2018, Author: MM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today aiming to sort out FISTA and NIHT. Observing and running FISTA it seems that the sparse coding step is sort of working... with each iteration of FISTA the l1 and l2 error seem to be decreasing. Across the six iterations of FISTA it seems big improvements are made, e.g. 1.3M at beginning for l2 error and 59K, sparsity level has also dropped to around 14%. Unfortunatly the dictionary update step does not seem to work. Increasing the learning rate helps significantly, but there seems to be this risk of FISTA still finding the zero solution. If the learning rate is increased the FISTA seems to find the zero vector on the second batch... Why might this be true? Fund a bug, had not updated the argument sequence so was recieving a tau of around\n",
    "\n",
    "What seems tricky about these approaches is that it is hard to tell weather you have just coded somethign poorly or weather the hyperparameters are just a way off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date 27/04/2018, Author: MM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have tried to implement OMP, with the X update solved via SGD. Two problems that I have observed are:\n",
    "- Firstly one cannot select to apply a filter at a specific location, the conv operator applies any filter in its weights across all locations. This is not an issue currently as we are using fully connected layers but we might want to think about it in future\n",
    "- Secondly the residue as each filter is added is barely reducing. This is probably because all the filters at this stage are pretty much random noise, and therefore none correlate very highly with it. The atom's contribution has not been entirely removed though unfortunatly. It seems to therefoe re-select this atom when choosing the next atom. If wee solved the OMP properly this shouldn't happen since the residue is orthogonal to the support dictionary. In other words the residue is in the Null space of the support dictionary and is hence orthogonal to the column space of the support dictionary.\n",
    "\n",
    "Second issue is the one I am trying to resolve. Interestingly the SGD seems to have converged to a loss of around 0.065; it does not seem to be the case that it is just that we are not properly solving for x and hence some of the residual is in part still present next iteration to correlate highly with the same atom again. Indeed, if this were the case then SGD would still to be seen to be decreasing. As a result it must be that there is another issue occurring... On analysis the restructured version Y formed from the support x and support dictionary is different from the full dictionary and the support x embedded in a tensor that is mostly zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date: 30/04/2018, Author: MM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far today have added Jere's more efficient hard threshold implementation for the IHT, have soft started the filter updates by running 5 updates without any sparse coding step, and have implemented a slow reduction in the sparsity constraint over time. Have also lowered the learning rate, previously was very high and suspect this meant the following:\n",
    "- was getting stuck in a plateau, and weight change step was to large to decrease cost function further\n",
    "- was obsevably getting large fluctuations between different batches, meaning the weights were swining around to much (although given how large the batch size was not totally convinced of this, MNIST is also highly redundant)\n",
    "\n",
    "Am now running on IHT and with the above changes seems to be behaving quite nicely in terms of the average l2 error per batch is consistently decreasing. Some outstanding issues still seem to be though:\n",
    "- IHT step seems to reduce the l2 error vey little, each step seems to make no change to the l2 error\n",
    "- reduction in l2 error although consistent is quite slow, at batch 46 of 60 still have erro of nearly 50%... Should maybe try and employ a learning rate that changes overtime, e.g. gears up and down depending on the change in error ove the last few batches. Need to see how we can do this in Pytoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date 01/04/2018, Author MM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus is to fix the simplest implementation, i.e. the IHT and get this working well. Observation is that almost straight away the sparse coding step makes almost no difference, is not able to decrease the error. Which is very odd, it implies that for a new batch, the current sparse code is as good as possible which just cannot be correct. Note this does not always seem to be the case, sometimes it seems to work quite well. Some ideas for what might be at fault:\n",
    "- maybe we are accidently feeding in the same batch time after time? *Do not think this is the problem, have tried on the fullbatch and still seems to be a problem*\n",
    "- is the momentum and nesterov side of things being messed up by the normalisation? Need to ensure that these are zeroed as it where after the end of each dictionary update cycle. *May still be an issue, but not source of current proble, set it to zero and made no difference*\n",
    "- is the step size in the wrong range, i.e. does it need to be larger or smaller? *Dosn't seem to matter how large I make the range, the linesearch still counts out*\n",
    "- is the gradient being calculated correctly?\n",
    "- is something odd happening with the computational graph? Need to check the detatch statements\n",
    "\n",
    "Observing the funtion running it is clear that it cannot find a good step size, indeed it nearly always runs down to the final possivle value of $\\alpha$.One correction was to Jere's function, had forgotten to compare threshold to the absolute value; however problem is still not fixed. After a fiar amount of testing I am convinced the hard thresholding function is correct. Test tried is to remnove the linesearch section, and instead just fix a step size and update. Interestingly the result of this was that one does observe a reduction in error over the iterations, but the between batch error is almost always jumps up to around 100% between each batch. Whereas with linesearch, one does not great any in batch improvement but the inter batch does improve better. This is strange, is it possible that just the first iteration or one step hard thresholding gets the optimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date 02/04/2018, Author MM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about what we should expect from our algorithm, and considering IHT as the simplest case, with each new batch the error should be 100% pretty much at the beginning. This is because our initial gues is just the zero vector and hence the l2 error is just the l2 norm of Y. Now as we perform a sparse coding push, what we should observe is that we should see the l2 error over the course of the sparse error step decrease, always down from 100% down to some lower level. Now the key aspect is that as we process batches, we should expect the value it converges to to get lower and lower as we learn a better and better dictionary, which in turn makes the job of the sparse coding step easier and easier. We therefore probably want to have a very low/small step at the beginning but as training proceeds it is probably a good thing if we can increase this value.\n",
    "\n",
    "With the linesearch model we were simply not getting any decrease in the error during the sparse coding step, which was really quite odd, now with the fixed sizes this is happening a lot better. Key is to run sparse coding for as long as possible, i.e. find as good a sparse representation as possible. Problem with IHT is that need to choose the step size in such a way to avoid it blowing up which it can quite easily do. The linesearch method should be good at handling this as should choose a step that always results in a decrease but dosn't seem to be working due to implementation. Also need to choose the dictionary step size and number of steps quite carefully, don't want to overfit it to each batch, but equally need the dictionary to sufficiently to give the sparse coding algorithm something to work with.\n",
    "\n",
    "Think the IHT is working now sort of as it should, however the dictionary update step becomes very slow towards the end of the first epoch. Worried about a number of things:\n",
    "- firstly I still only think a few filters are being learned. Need to implement some sort of dropout equivalent\n",
    "- no longer using the linesearch and don't see why this isn't working\n",
    "- need to implement some logging steps\n",
    "\n",
    "Have just realised had momentum and Nesterov off! Seems to have sped thiongs up significantly, probably still best to check the momentum business by redefining the optimizer everytime it finishes, and in doing so ensuring that the momentum is removed. Think the big challenge is the fact am only updating a few of the filters. Ideas for how to tackle this:\n",
    "- slower decay time: not sure this will work, key thing required is that we need to train lots of globabl filters rather than lots of local ones\n",
    "- some form of dropout, this is the only way I can see that we can ensure lots of global filters are trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-env]",
   "language": "python",
   "name": "conda-env-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
