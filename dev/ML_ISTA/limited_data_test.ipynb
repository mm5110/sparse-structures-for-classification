{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limited data test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to provide a rig to test and compare the performance of the bench mark NN, ML-ISTA and ML-JISTA network in terms of their ability to efficiently learn weight parameters to minimise empirical test error. Testing involves training the a number of these networks with varying numbers of training data points made available. Idea is perhaps that JISTA can act as a 'camel' in terms of leading fewer training data points to do a good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import time\n",
    "import progressbar\n",
    "import importlib\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "import pdb\n",
    "\n",
    "import Models_MNIST as mds\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 80\n",
    "BATCH_SIZE = 256\n",
    "DOWNLOAD_MNIST = False\n",
    "m1 = 64\n",
    "m2 = 128\n",
    "m3 = 512\n",
    "cudaopt = True\n",
    "\n",
    "EPS = 1e-4\n",
    "\n",
    "# numb_train = [512, 1024, 4096, 10000, 20000, 60000]\n",
    "numb_train = [256, 512, 1024, 2048, 4096, 8192, 10000]\n",
    "train_data_sets = []\n",
    "\n",
    "# Create different size training sets for the the training data\n",
    "for i in range(len(numb_train)):\n",
    "    # Mnist digits dataset\n",
    "    train_data_all = torchvision.datasets.MNIST(\n",
    "        root='../data',\n",
    "        train=True,                                     # this is training data\n",
    "        transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to                                                       # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "        download=True,                        # download it if you don't have it\n",
    "    )\n",
    "    \n",
    "    train_data = train_data_all\n",
    "    train_set = np.random.choice(60000, int(numb_train[i]))\n",
    "    train_data.train_data = train_data_all.train_data[torch.LongTensor(train_set),:,:] \n",
    "    train_data.train_labels = train_data.train_labels[torch.LongTensor(train_set)]\n",
    "    train_data_sets.append(train_data)\n",
    "\n",
    "# Download and load in the test data\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='../data',\n",
    "    train=False,                                     # this is testing data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=True,                        # download it if you don't have it\n",
    ")\n",
    "\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f149523af98>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,EPOCH,EPOCH)\n",
    "Rhos = 1/(1+np.exp(-(x- EPOCH*6/9 )*.2))\n",
    "plt.plot(Rhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\t\tTraining Baseline \n",
      "\n",
      "Training and testing models with training data size of 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ |#                                                  | 0 Elapsed Time: 0:00:00/anaconda/envs/mlvm/lib/python3.6/site-packages/ipykernel/__main__.py:112: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/mlvm/lib/python3.6/site-packages/ipykernel/__main__.py:117: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/mlvm/lib/python3.6/site-packages/ipykernel/__main__.py:122: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda/envs/mlvm/lib/python3.6/site-packages/ipykernel/__main__.py:127: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/ |   #                                              | 41 Elapsed Time: 0:11:19"
     ]
    }
   ],
   "source": [
    "importlib.reload(mds)\n",
    "\n",
    "T = 4\n",
    "\n",
    "Loss_test_baseline = np.zeros((EPOCH, len(numb_train)))\n",
    "Acc_test_baseline = np.zeros((EPOCH, len(numb_train)))\n",
    "\n",
    "Loss_test_ISTA = np.zeros((EPOCH, len(numb_train)))\n",
    "Acc_test_ISTA = np.zeros((EPOCH, len(numb_train)))\n",
    "\n",
    "Loss_test_JISTA_0 = np.zeros((EPOCH, len(numb_train)))\n",
    "Acc_test_JISTA_0 = np.zeros((EPOCH, len(numb_train)))\n",
    "\n",
    "Loss_test_JISTA = np.zeros((EPOCH, len(numb_train)))\n",
    "Acc_test_JISTA = np.zeros((EPOCH, len(numb_train)))\n",
    "\n",
    "print('\\n\\t\\t\\t\\t\\tTraining Baseline \\n')\n",
    "\n",
    "# Iterate through each of the training data set sizes\n",
    "for i in range(len(numb_train)):\n",
    "    print(\"Training and testing models with training data size of \" + str(int(numb_train[i])))\n",
    "    # Load data for the correct size\n",
    "    train_loader = Data.DataLoader(dataset=train_data_sets[i], batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Define the three models we want to compare     \n",
    "    model_baseline = mds.ML_ISTA_NET(m1,m2,m3)\n",
    "    model_ISTA = mds.ML_ISTA_NET(m1,m2,m3)\n",
    "    model_JISTA_0 = mds.ML_JISTA_NET(m1,m2,m3)\n",
    "    model_JISTA = mds.ML_JISTA_NET(m1,m2,m3)\n",
    "    \n",
    "    # Optimise if CUDA is available   \n",
    "    if cudaopt:\n",
    "        model_baseline.cuda()\n",
    "        model_ISTA.cuda()\n",
    "        model_JISTA_0.cuda()\n",
    "        model_JISTA.cuda()\n",
    "    \n",
    "    # Define optimizer for each model to update a specific model's parameters    \n",
    "    optimizer_baseline = torch.optim.Adam(model_baseline.parameters(), lr = 0.0001, eps = EPS)\n",
    "    optimizer_ISTA = torch.optim.Adam(model_ISTA.parameters(), lr = 0.0001, eps = EPS)\n",
    "    optimizer_JISTA_0 = torch.optim.Adam(model_JISTA_0.parameters(), lr = 0.0001, eps = EPS)\n",
    "    optimizer_JISTA = torch.optim.Adam(model_JISTA.parameters(), lr = 0.0001, eps = EPS)\n",
    "    \n",
    "    bar = progressbar.ProgressBar()\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "\n",
    "        bar.update((epoch+1)/EPOCH*100)\n",
    "        RHO =  float(Rhos[epoch])\n",
    "        # train each model for 1 epoch\n",
    "        model_baseline.train()\n",
    "        model_ISTA.train()\n",
    "        model_JISTA_0.train()\n",
    "        model_JISTA.train()\n",
    "        \n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            b_x = Variable(x)   # batch x, shape (batch, 28*28)\n",
    "            b_y = Variable(y)               # batch label\n",
    "            if cudaopt:\n",
    "                b_y, b_x = b_y.cuda(), b_x.cuda()\n",
    "            # Run data through each model                \n",
    "            encoded_baseline, scores_baseline = model_baseline(b_x)\n",
    "            encoded_ISTA, scores_ISTA = model_ISTA(b_x, T, RHO)\n",
    "            encoded_JISTA_0, scores_JISTA_0, sorted_labels_JISTA_0 = model_JISTA_0.joint_train(b_x, b_y)\n",
    "            encoded_JISTA, scores_JISTA, sorted_labels_JISTA = model_JISTA.joint_train(b_x, b_y, T, RHO)\n",
    "            # Calculate the training loss for each model and backprop\n",
    "            # BASELINE            \n",
    "            loss_baseline = F.nll_loss(scores_baseline, b_y)      # negative log likelyhood\n",
    "            optimizer_baseline.zero_grad()               # clear gradients for this training step\n",
    "            loss_baseline.backward()                     # backpropagation, compute gradients\n",
    "            optimizer_baseline.step()                    # apply gradients\n",
    "            # ISTA            \n",
    "            loss_ISTA = F.nll_loss(scores_ISTA, b_y)      # negative log likelyhood\n",
    "            optimizer_ISTA.zero_grad()               # clear gradients for this training step\n",
    "            loss_ISTA.backward()                     # backpropagation, compute gradients\n",
    "            optimizer_ISTA.step()                    # apply gradients\n",
    "            # JISTA_0            \n",
    "            sorted_labels_JISTA_0 = sorted_labels_JISTA_0.type(torch.cuda.LongTensor)           \n",
    "            loss_JISTA_0 = F.nll_loss(scores_JISTA_0, sorted_labels_JISTA_0)      # negative log likelyhood\n",
    "            optimizer_JISTA_0.zero_grad()               # clear gradients for this training step\n",
    "            loss_JISTA_0.backward()                     # backpropagation, compute gradients\n",
    "            optimizer_JISTA_0.step()                    # apply gradients\n",
    "            # JISTA            \n",
    "            sorted_labels_JISTA = sorted_labels_JISTA.type(torch.cuda.LongTensor)          \n",
    "            loss_JISTA = F.nll_loss(scores_JISTA, sorted_labels_JISTA)      # negative log likelyhood\n",
    "            optimizer_JISTA.zero_grad()               # clear gradients for this training step\n",
    "            loss_JISTA.backward()                     # backpropagation, compute gradients\n",
    "            optimizer_JISTA.step()                    # apply gradients\n",
    "        \n",
    "        # TEST EACH MODEL AND STORE PERFORMANCE\n",
    "        model_baseline.eval()\n",
    "        model_ISTA.eval()\n",
    "        model_JISTA_0.eval() \n",
    "        model_JISTA.eval()     \n",
    "        correct_baseline = 0\n",
    "        correct_ISTA = 0\n",
    "        correct_JISTA_0 = 0\n",
    "        correct_JISTA = 0       \n",
    "        test_loss_baseline = 0\n",
    "        test_loss_ISTA = 0\n",
    "        test_loss_JISTA_0 = 0\n",
    "        test_loss_JISTA = 0\n",
    "        \n",
    "        for step, (x, y) in enumerate(test_loader):\n",
    "            b_x = Variable(x)   # batch x, shape (batch, 28*28)\n",
    "            b_y = Variable(y)               # batch label\n",
    "            if cudaopt:\n",
    "                b_y, b_x = b_y.cuda(), b_x.cuda()\n",
    "                \n",
    "            # Calculate the test loss for each model\n",
    "            gamma, scores = model_baseline(b_x)\n",
    "            test_loss_baseline += F.nll_loss(scores, b_y, size_average=False).data[0]\n",
    "            pred = scores.data.max(1, keepdim=True)[1]\n",
    "            correct_baseline += pred.eq(b_y.data.view_as(pred)).long().cpu().sum()\n",
    "            \n",
    "            gamma, scores = model_ISTA(b_x,T,RHO)\n",
    "            test_loss_ISTA += F.nll_loss(scores, b_y, size_average=False).data[0]\n",
    "            pred = scores.data.max(1, keepdim=True)[1]\n",
    "            correct_ISTA += pred.eq(b_y.data.view_as(pred)).long().cpu().sum()\n",
    "            \n",
    "            gamma, scores = model_JISTA_0(b_x)\n",
    "            test_loss_JISTA_0 += F.nll_loss(scores, b_y, size_average=False).data[0]\n",
    "            pred = scores.data.max(1, keepdim=True)[1]\n",
    "            correct_JISTA_0 += pred.eq(b_y.data.view_as(pred)).long().cpu().sum()\n",
    "            \n",
    "            gamma, scores = model_JISTA(b_x,T,RHO)\n",
    "            test_loss_JISTA += F.nll_loss(scores, b_y, size_average=False).data[0]\n",
    "            pred = scores.data.max(1, keepdim=True)[1]\n",
    "            correct_JISTA += pred.eq(b_y.data.view_as(pred)).long().cpu().sum()\n",
    "        \n",
    "        # Calculate each error as a percentage\n",
    "        test_loss_baseline /= len(test_loader.dataset)\n",
    "        test_loss_ISTA /= len(test_loader.dataset)\n",
    "        test_loss_JISTA_0 /= len(test_loader.dataset)\n",
    "        test_loss_JISTA /= len(test_loader.dataset)\n",
    "        \n",
    "        Loss_test_baseline[epoch,i] = test_loss_baseline\n",
    "        Acc_test_baseline[epoch,i] =  100 * float(correct_baseline) /float(len(test_loader.dataset))\n",
    "        \n",
    "        Loss_test_ISTA[epoch,i] = test_loss_ISTA\n",
    "        Acc_test_ISTA[epoch,i] =  100 * float(correct_ISTA) /float(len(test_loader.dataset))\n",
    "        \n",
    "        Loss_test_JISTA_0[epoch,i] = test_loss_JISTA_0\n",
    "        Acc_test_JISTA_0[epoch,i] =  100 * float(correct_JISTA_0) /float(len(test_loader.dataset))\n",
    "        \n",
    "        Loss_test_JISTA[epoch,i] = test_loss_JISTA\n",
    "        Acc_test_JISTA[epoch,i] =  100 * float(correct_JISTA) /float(len(test_loader.dataset))\n",
    "\n",
    "#     torch.save(model.state_dict(), 'cnn_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs number of data points after 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.style.use('default')\n",
    "plt.plot(numb_train, Acc_test_baseline[EPOCH-1, :],  linewidth = 2,label='baseline')\n",
    "plt.plot(numb_train, Acc_test_ISTA[EPOCH-1, :], linewidth = 2,label = 'ML-ISTA')\n",
    "plt.plot(numb_train, Acc_test_JISTA_0[EPOCH-1, :], linewidth = 2,label = 'ML-JISTA_0')\n",
    "plt.plot(numb_train, Acc_test_JISTA[EPOCH-1, :], linewidth = 2,label = 'ML-JISTA')\n",
    "# plt.plot(Acc_test_fista_r, linewidth = 2,label = 'ML-FISTA')\n",
    "\n",
    "plt.grid('on')\n",
    "plt.title('Test Accuracy - 4 Unfoldings, 100 Epochs')\n",
    "plt.xlabel('Number of training data points')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.legend()\n",
    "plt.axis([0, 10000, 0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.style.use('default')\n",
    "plt.plot(Acc_test_baseline[:, idx],  linewidth = 2,label='baseline')\n",
    "plt.plot(Acc_test_ISTA[:, idx], linewidth = 2,label = 'ML-ISTA')\n",
    "plt.plot(Acc_test_JISTA_0[:, idx], linewidth = 2,label = 'ML-JISTA_0')\n",
    "plt.plot(Acc_test_JISTA[:, idx], linewidth = 2,label = 'ML-JISTA')\n",
    "# plt.plot(Acc_test_fista_r, linewidth = 2,label = 'ML-FISTA')\n",
    "\n",
    "plt.grid('on')\n",
    "plt.title('Test accuracy vs number of epochs - 4 Unfoldings, 4096 Training Points')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.legend()\n",
    "plt.axis([0, 100, 0, 100])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlvm]",
   "language": "python",
   "name": "conda-env-mlvm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
